import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.datasets import fashion_mnist

(train, trainLabel), (test, testLabel) = fashion_mnist.load_data()

# look at the data shape
print(train.shape, trainLabel.shape)
# shows 60000 images in data set, each image is expressed i 28 by 28 pixel grid
# there 60000 labels

# look at the first 5 labels in the training dataset 
print([trainLabel[i] for i in range(5) ])

# view the first image
import matplotlib.pyplot as plt

plt.figure()
plt.imshow(train[0])

# the largest pixel value is 255 so we can stanardize the data by dividing all measures by 255
trainScaled = train/255.0
testScaled = test/255.0

# our neural network will have 784 neurons in the input layer (one for each variable)
# and 10 neurons ing the output layer (one for eah clothing item classification)


# Create Model 
model = keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)), # this is the first layer being converted to simgle layer. Flatten helps by converting th 2d array to 1D 
    tf.keras.layers.Dense(128, activation = 'relu'), # first ddense layer. uses the relu function. 128 is the number of neurons
    tf.keras.layers.Dense(10, activation='softmax') #output layer. uses the softmax function. 10 is the number of neurons 
    ])

# Compile the model 
# specify the loss funciton and the optimizer 
# the adam optimizer is stochastic gradient descent 
# sparse category cross entropy means multiple category classification
# binary would be used for two classes 
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', 
              metrics='accuracy')


model.summary()


# train the model with the fit function
# epoch is the number of times training is repeated 
# accuracy improves for each epoch
history = model.fit(trainScaled, trainLabel, epochs=5)

# how do we know the best value for epoch?
# a large epoch  does not necesarrily mean the best loss and accuracy 
# we can determine the optimal epoch by storing and viewing indices of the  model.fit history

# this shows that the loss and accuracy for each epoch is stored in the instance named history 
print(history.history.keys())

# view where the diminishing returns are for loss
import matplotlib.pyplot as plt 
plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()

# When we went from 5 to 20 epochs performance increased
# above was evaluating performance on the training data. 
# We need to see if the loss improved in the test data as well.
history2 = model.fit(trainScaled, trainLabel, epochs=20)

plt.plot(history2.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

plt.plot(history2.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()

# evaluate with the test set 
history3 = model.fit(trainScaled, trainLabel, epochs = 20,
                     validation_data=(testScaled, testLabel))

plt.plot(history3.history['loss'])
plt.plot(history3.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'])
plt.show()

# create a new model with dropout 
model2 = keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28,28)),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.Dropout(0.3),         # specifies 30% of nodes to be dropped
    tf.keras.layers.Dense(10, activation='softmax')
    ])

# compile the model 
model2.compile(loss='sparse_categorical_crossentropy',
               metrics = 'accuracy')
history4 = model2.fit(trainScaled, trainLabel, epochs=20,
                      validation_data=(testScaled, testLabel))

plt.plot(history4.history['loss'])
plt.plot(history4.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'test'])
plt.show
